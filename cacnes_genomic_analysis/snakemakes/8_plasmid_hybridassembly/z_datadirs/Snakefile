##########################################
# RAVALIKA SNAKEFILE FOR HYBRID ASSEMBLY #
##########################################

# Reminders:
# # Put your own email address in cluster.slurm.json and myjob.slurm so that you get emails about failures. 



''' SETUP '''

# csv file listing sample names and locations of raw data
spls = "samples.csv"

import sys
SCRIPTS_DIRECTORY = "/scratch/mit_lieberman/projects/aro_cacnes_biogeo/lab_scripts"
sys.path.insert(0, SCRIPTS_DIRECTORY)

from read_samplesCSV_nanopore import *



''' PRE-SNAKEMAKE '''

## Define couple of lists from samples.csv
## Format: Path,Sample,ReferenceGenome,ProviderName,Subject
[PATH_Illumina_ls, PATH_Nanopore_ls, SAMPLE_ls, REF_Genome_ls, PROVIDER_ls, BARCODE_ls, NumNpFiles_ls, CLADEID_ls] = read_samplesCSV(spls)
# Write sample_info.csv for each sample
split_samplesCSV(PATH_Illumina_ls,PATH_Nanopore_ls,SAMPLE_ls,REF_Genome_ls,PROVIDER_ls,BARCODE_ls,NumNpFiles_ls, CLADEID_ls)
CLADES_ls = set(CLADEID_ls)



''' SNAKEMAKE '''

rule all:
    input:
    	# # Only data links # #
    	expand("data/{sampleID}/R1.fq.gz",sampleID=SAMPLE_ls),
    	expand("data/{sampleID}/R2.fq.gz",sampleID=SAMPLE_ls),
    	expand("data/{sampleID}/np.fq.gz",sampleID=SAMPLE_ls),   #using fastq_pass files for long reads - no additional processing
    	# # Through Illumina processing # #
    	expand("1-illumina_processed/{sampleID}/filt1.fq.gz",sampleID=SAMPLE_ls),
    	expand("1-illumina_processed/{sampleID}/filt2.fq.gz",sampleID=SAMPLE_ls),
    	expand("1-illumina_processed/{sampleID}/filt_sgls.fq.gz",sampleID=SAMPLE_ls),
    	# # Through Unicycler # # hybrid assembly
    	expand("3-unicycler/sample_{sampleID}/assembly.fasta",sampleID=SAMPLE_ls),
		# # # Through Prokka # # annotate genome
    	expand("4-annotation/sample_{sampleID}/prokka_out.txt",sampleID=SAMPLE_ls),
     	# # Through BLAST # # 
    	expand("6-blast_plasmid/sample_{sampleID}/{plasmidID}_plasmidhits.csv", sampleID=SAMPLE_ls, plasmidID = ["CP003294","CP017041","region_A","region_B","region_C","region_common","region_prophage","PPA1294_Cacnes_gene"]),
		# Including cleanup # #
		#"logs/cleanUp_done.txt",

rule make_data_links:
	input:
		sample_info_csv="data/{sampleID}/sample_info.csv",
	output:
		# Recommend using symbolic links to your likely many different input files
		fq1="data/{sampleID}/R1.fq.gz",
		fq2="data/{sampleID}/R2.fq.gz",
		fq_np_cat="data/{sampleID}/np.fq.gz"
	run:
		# get stuff out of mini csv file
		with open(input.sample_info_csv,'r') as f:
			this_sample_info = f.readline() # only one line to read
		this_sample_info = this_sample_info.strip('\n').split(',')
		PATH_Illumina = this_sample_info[0] # remember python indexing starts at 0
		paths = PATH_Illumina.split(' ')
		PATH_Nanopore = this_sample_info[1]
		sample = this_sample_info[2]
		provider = this_sample_info[4]
		barcode = this_sample_info[5]
		numnpfiles = this_sample_info[6]
		# make links
		if len(paths)>1:
			cp_append_files(paths, sample, provider)
		else:
			makelink(PATH_Illumina, sample, provider)
		cp_append_nanopore_files(PATH_Nanopore,sample,barcode,numnpfiles)  # concatenates and zips nanopore fastq_pass files for Unicycler


## Illumina processing (short reads)

rule cutadapt:
	input:
		# Recommend using symbolic links to your likely many different input files
		fq1 = rules.make_data_links.output.fq1,
		fq2 = rules.make_data_links.output.fq2,
	output:
		fq1o="0-tmp/{sampleID}_R1_trim.fq.gz",
		fq2o="0-tmp/{sampleID}_R2_trim.fq.gz",
	conda:
		"envs/cutadapt.yaml"
	shell:
		"cutadapt -a CTGTCTCTTAT -o {output.fq1o} {input.fq1} ;"
		"cutadapt -a CTGTCTCTTAT -o {output.fq2o} {input.fq2} ;"


rule sickle2050:
	input:
		fq1o = rules.cutadapt.output.fq1o,
		fq2o = rules.cutadapt.output.fq2o,
	output:
		fq1o="1-illumina_processed/{sampleID}/filt1.fq.gz",
		fq2o="1-illumina_processed/{sampleID}/filt2.fq.gz",
		fqSo="1-illumina_processed/{sampleID}/filt_sgls.fq.gz",
	conda:
		"envs/sickle-trim.yaml"
	shell:
		"sickle pe -g -f {input.fq1o} -r {input.fq2o} -t sanger -o {output.fq1o} -p {output.fq2o} -s {output.fqSo} -q 20 -l 50 -x -n"


# Nanopore proessing (long reads)

rule filtlong:  # Removes reads shorter than 20kb to limit data and make Unicycler run faster
	input:
		np_input = rules.make_data_links.output.fq_np_cat,
	output:
		np_output_trimmed="2-nanopore-processed/{sampleID}/np_trimmed.fq.gz",
	conda:
		"envs/filtlong.yaml"
	shell:
		"filtlong --min_length 20000 --keep_percent 99 --target_bases 500000000 {input.np_input} | gzip > {output.np_output_trimmed}"


# Hybrid Assembly and Annotation

rule unicycler:
	input:
		illumina_1 = rules.sickle2050.output.fq1o,
		illumina_2 = rules.sickle2050.output.fq2o,
		nanopore = rules.filtlong.output.np_output_trimmed,
		# nanopore = rules.porechop.output.np_pc_output   #if taking from porechop
	params:
		threads=16,
		outdir="3-unicycler/sample_{sampleID}"
	output:
		genome = "3-unicycler/sample_{sampleID}/assembly.fasta"
	conda:
		"envs/hybrid_assembly_unicycler.yaml"
	shell:
		"unicycler -1 {input.illumina_1} -2 {input.illumina_2} -l {input.nanopore} -o {params.outdir} --verbosity 3 -t {params.threads}"

rule prokka:
	# prokka annotations; not currently using --proteins option to specify trusted gbk file
	input:
		genome=rules.unicycler.output.genome,
	params:
		outdir="4-annotation/sample_{sampleID}",
	threads: 16
	output:
		"4-annotation/sample_{sampleID}/prokka_out.txt",
	conda:
		"envs/prokka_new.yaml", #prokka
	shell:
		"prokka --force --cpus {threads} --mincontiglen 500 --outdir {params.outdir} --prefix prokka_out {input.genome} ; conda deactivate"


## BLAST Analysis

rule make_blastdb:
    input:
        genome=rules.unicycler.output.genome,
    output:
        dbscaffolds="5-blastDB/sample_{sampleID}/blast-db.nhr", # a file type generated by makeblastdb
    params:
        out_tag="5-blastDB/sample_{sampleID}/blast-db",
    shell:
        " module add c3ddb/blast+/2.7.1 ;"
        " makeblastdb -in {input.genome} -parse_seqids -title sample_{wildcards.sampleID} -dbtype nucl -out {params.out_tag} ; "

rule blast_plasmid:
	input:
		dbscaffolds=rules.make_blastdb.output.dbscaffolds, 
	output: 
		plasmid_hits="6-blast_plasmid/sample_{sampleID}/{plasmidID}_plasmidhits.csv",
	params:
		out_tag="5-blastDB/sample_{sampleID}/blast-db",
		fasta_plasmid="ref_plasmid/{plasmidID}.fasta",
	shell: 
		"module add c3ddb/blast+/2.7.1;" 
		"blastn -db {params.out_tag} -query {params.fasta_plasmid} -outfmt 10 -out {output.plasmid_hits} -max_hsps 100 ; "

