#####################################
# SNAKEFILE FOR PROCESSING RAW DATA #
#####################################

# Reminders:
# # Put your own email address in cluster.slurm.json so that you get emails about failures. No one else wants your slurm emails.



''' VARIABLES '''

# csv file listing all samples names and locations of raw data
spls = "samples.csv"

import sys
SCRIPTS_DIRECTORY = "/scratch/mit_lieberman/projects/aro_cacnes_biogeo/lab_scripts"
sys.path.insert(0, SCRIPTS_DIRECTORY)

from read_move_link_samplesCSV import *



''' PRE-SNAKEMAKE '''

## define couple of lists from samples.csv
## Format: Path,Sample,ReferenceGenome,ProviderName,Subject
[PATH_ls, SAMPLE_ls, REF_Genome_ls, PROVIDER_ls, PATIENTID_ls_all] = read_samplesCSV(spls)
PATIENTID_ls = set(PATIENTID_ls_all) # set(list_patient) provides only unique subject IDs
## Write sample_info.csv for each sample
split_samplesCSV(PATH_ls,SAMPLE_ls,REF_Genome_ls,PROVIDER_ls,PATIENTID_ls_all)



''' SNAKEMAKE '''

rule all:
    input:
    	# # Only data links # #
    	# expand("data/{sampleID}/R1.fq.gz",sampleID=SAMPLE_ls),
    	# expand("data/{sampleID}/R2.fq.gz",sampleID=SAMPLE_ls),
    	# # Through cutadapt # #
    	expand("1-cutadapt/{sampleID}_R1_trim.fq.gz",sampleID=SAMPLE_ls),
    	expand("1-cutadapt/{sampleID}_R2_trim.fq.gz",sampleID=SAMPLE_ls),
    	# # Through all steps # #
    	expand("2-sickle2050/{sampleID}_filt1.fq.gz",sampleID=SAMPLE_ls),
    	expand("2-sickle2050/{sampleID}_filt2.fq.gz",sampleID=SAMPLE_ls),
    	expand("2-sickle2050/{sampleID}_filt_sgls.fq.gz",sampleID=SAMPLE_ls),



rule make_data_links:
	# NOTE: All raw data needs to be names fastq.gz. No fq! The links will be names fq though.
	input:
		sample_info_csv="data/{sampleID}/sample_info.csv",
	output:
 		# Recommend using symbolic links to your likely many different input files
 		fq1="data/{sampleID}/R1.fq.gz",
 		fq2="data/{sampleID}/R2.fq.gz",
	run:
		# get stuff out of mini csv file
		with open(input.sample_info_csv,'r') as f:
			this_sample_info = f.readline() # only one line to read
		this_sample_info = this_sample_info.strip('\n').split(',')
		path = this_sample_info[0] # remember python indexing starts at 0
		paths = path.split(' ')
		sample = this_sample_info[1]
		provider = this_sample_info[3]
		# make links
		if len(paths)>1:
			cp_append_files(paths, sample, provider)
		else:
			makelink(path, sample, provider)


rule cutadapt:
	input:
		fq1=rules.make_data_links.output.fq1,
		fq2=rules.make_data_links.output.fq2,
	output:
		fq1o="1-cutadapt/{sampleID}_R1_trim.fq.gz",
		fq2o="1-cutadapt/{sampleID}_R2_trim.fq.gz",
	conda:
		"envs/cutadapt.yaml",
	shell:
		"cutadapt -a CTGTCTCTTAT -o {output.fq1o} {input.fq1} ;"
		"cutadapt -a CTGTCTCTTAT -o {output.fq2o} {input.fq2} ;"


rule sickle2050:
	input:
		fq1=rules.cutadapt.output.fq1o,
		fq2=rules.cutadapt.output.fq2o,
	output:
		fq1o="2-sickle2050/{sampleID}_filt1.fq.gz",
		fq2o="2-sickle2050/{sampleID}_filt2.fq.gz",
		fqSo="2-sickle2050/{sampleID}_filt_sgls.fq.gz",
	conda:
		"envs/sickle-trim.yaml", # sickle	
	shell:
		"sickle pe -g -f {input.fq1} -r {input.fq2} -t sanger -o {output.fq1o} -p {output.fq2o} -s {output.fqSo} -q 20 -l 50 -x -n"

